{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my previous post, I showed some word statistics, common phrases, and topics found with LDA and LSI. Yet, it wasn't the end of what I want to do. As with any other bit of remotely expensive, unusual consumer equipment (electron microscopes, bread makers), I feel the need to not only use my GPU at every possible opportunity, but take every chance to tell people that I used it.\n",
    "I have the fascination with word vectors that comes with a cognitive science background, and at some point in the past, I got the idea of training word vectors on a corpus, and using the clusters to model topics. That was with word2vec, then. Since I don't want to reinvent the wheel, I found this package [link: https://github.com/MaartenGr/BERTopic], which uses BERT embeddings to cluster the topics in your dataset, and then make a visualization with Plotly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are an attempt to represent the semantic meaning of a word in n-dimensional space, and they do so faithfully enough that you can do algebra with them. The example of \"King - man + woman = Queen\" has been repeated enough times to become reyified - stripped of its meaning by frequent use. But, it's an example of how the word can be represented as a point in space.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
