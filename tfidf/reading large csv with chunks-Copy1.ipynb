{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_csv(r\"D:\\data\\openparliament\\statements_nospeaker_en.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.drop('Unnamed: 0',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>politician_id</th>\n",
       "      <th>riding_id</th>\n",
       "      <th>party_id</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_date</th>\n",
       "      <th>id.1</th>\n",
       "      <th>document_id</th>\n",
       "      <th>time</th>\n",
       "      <th>h1_en</th>\n",
       "      <th>...</th>\n",
       "      <th>slug</th>\n",
       "      <th>urlcache</th>\n",
       "      <th>h1_fr</th>\n",
       "      <th>h2_fr</th>\n",
       "      <th>h3_fr</th>\n",
       "      <th>who_fr</th>\n",
       "      <th>who_context_fr</th>\n",
       "      <th>wordcount_en</th>\n",
       "      <th>tokenized_content</th>\n",
       "      <th>slug_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2611</td>\n",
       "      <td>3465</td>\n",
       "      <td>35066</td>\n",
       "      <td>4</td>\n",
       "      <td>2005-11-29</td>\n",
       "      <td>1997-09-22</td>\n",
       "      <td>232373</td>\n",
       "      <td>1878</td>\n",
       "      <td>2001-05-03 13:50:00-04</td>\n",
       "      <td>Government Orders</td>\n",
       "      <td>...</td>\n",
       "      <td>marlene-catterall-1</td>\n",
       "      <td>/debates/2001/5/3/marlene-catterall-1/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\N</td>\n",
       "      <td>['mr.', 'speaker', ',', 'i', 'rise', 'on', 'a'...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3210</td>\n",
       "      <td>3465</td>\n",
       "      <td>70224</td>\n",
       "      <td>4</td>\n",
       "      <td>1997-04-27</td>\n",
       "      <td>1994-01-17</td>\n",
       "      <td>232373</td>\n",
       "      <td>1878</td>\n",
       "      <td>2001-05-03 13:50:00-04</td>\n",
       "      <td>Government Orders</td>\n",
       "      <td>...</td>\n",
       "      <td>marlene-catterall-1</td>\n",
       "      <td>/debates/2001/5/3/marlene-catterall-1/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\N</td>\n",
       "      <td>['mr.', 'speaker', ',', 'i', 'rise', 'on', 'a'...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4305</td>\n",
       "      <td>173</td>\n",
       "      <td>70358</td>\n",
       "      <td>1</td>\n",
       "      <td>\\N</td>\n",
       "      <td>2015-10-19</td>\n",
       "      <td>645329</td>\n",
       "      <td>388</td>\n",
       "      <td>2008-02-14 13:15:00-05</td>\n",
       "      <td>Routine Proceedings</td>\n",
       "      <td>...</td>\n",
       "      <td>tom-lukiwski-7</td>\n",
       "      <td>/debates/2008/2/14/tom-lukiwski-7/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\N</td>\n",
       "      <td>['finally', ',', 'mr.', 'speaker', ',', 'i', '...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1534</td>\n",
       "      <td>173</td>\n",
       "      <td>47007</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-19</td>\n",
       "      <td>2004-10-04</td>\n",
       "      <td>645329</td>\n",
       "      <td>388</td>\n",
       "      <td>2008-02-14 13:15:00-05</td>\n",
       "      <td>Routine Proceedings</td>\n",
       "      <td>...</td>\n",
       "      <td>tom-lukiwski-7</td>\n",
       "      <td>/debates/2008/2/14/tom-lukiwski-7/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\N</td>\n",
       "      <td>['finally', ',', 'mr.', 'speaker', ',', 'i', '...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1541</td>\n",
       "      <td>43</td>\n",
       "      <td>47014</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-19</td>\n",
       "      <td>2004-10-04</td>\n",
       "      <td>313253</td>\n",
       "      <td>1621</td>\n",
       "      <td>2004-02-06 10:05:00-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>garry-breitkreuz-1</td>\n",
       "      <td>/debates/2004/2/6/garry-breitkreuz-1/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\N</td>\n",
       "      <td>['mr.', 'speaker', ',', 'i', 'rise', 'today', ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  politician_id  riding_id  party_id    end_date  start_date    id.1  \\\n",
       "0  2611           3465      35066         4  2005-11-29  1997-09-22  232373   \n",
       "1  3210           3465      70224         4  1997-04-27  1994-01-17  232373   \n",
       "2  4305            173      70358         1          \\N  2015-10-19  645329   \n",
       "3  1534            173      47007         1  2015-10-19  2004-10-04  645329   \n",
       "4  1541             43      47014         1  2015-10-19  2004-10-04  313253   \n",
       "\n",
       "   document_id                    time                h1_en  ...  \\\n",
       "0         1878  2001-05-03 13:50:00-04    Government Orders  ...   \n",
       "1         1878  2001-05-03 13:50:00-04    Government Orders  ...   \n",
       "2          388  2008-02-14 13:15:00-05  Routine Proceedings  ...   \n",
       "3          388  2008-02-14 13:15:00-05  Routine Proceedings  ...   \n",
       "4         1621  2004-02-06 10:05:00-05                  NaN  ...   \n",
       "\n",
       "                  slug                                urlcache h1_fr h2_fr  \\\n",
       "0  marlene-catterall-1  /debates/2001/5/3/marlene-catterall-1/   NaN   NaN   \n",
       "1  marlene-catterall-1  /debates/2001/5/3/marlene-catterall-1/   NaN   NaN   \n",
       "2       tom-lukiwski-7      /debates/2008/2/14/tom-lukiwski-7/   NaN   NaN   \n",
       "3       tom-lukiwski-7      /debates/2008/2/14/tom-lukiwski-7/   NaN   NaN   \n",
       "4   garry-breitkreuz-1   /debates/2004/2/6/garry-breitkreuz-1/   NaN   NaN   \n",
       "\n",
       "   h3_fr  who_fr  who_context_fr wordcount_en  \\\n",
       "0    NaN     NaN             NaN           \\N   \n",
       "1    NaN     NaN             NaN           \\N   \n",
       "2    NaN     NaN             NaN           \\N   \n",
       "3    NaN     NaN             NaN           \\N   \n",
       "4    NaN     NaN             NaN           \\N   \n",
       "\n",
       "                                   tokenized_content slug_length  \n",
       "0  ['mr.', 'speaker', ',', 'i', 'rise', 'on', 'a'...           3  \n",
       "1  ['mr.', 'speaker', ',', 'i', 'rise', 'on', 'a'...           3  \n",
       "2  ['finally', ',', 'mr.', 'speaker', ',', 'i', '...           3  \n",
       "3  ['finally', ',', 'mr.', 'speaker', ',', 'i', '...           3  \n",
       "4  ['mr.', 'speaker', ',', 'i', 'rise', 'today', ...           3  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk = pd.read_csv(r\"D:\\data\\openparliament\\statements_nospeaker_en.csv\", chunksize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_en_list = []  # append the content from each chunk df here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Each chunk is in df format\n",
    "for chunk in df_chunk:  \n",
    "    # perform data filtering \n",
    "    chunk_filter = chunk.drop(['Unnamed: 0'], axis=1)\n",
    "    chunk_filter = chunk.content_en\n",
    "    print(type(chunk_filter))\n",
    "    \n",
    "    # Once the data filtering is done, append the chunk to list\n",
    "    content_en_list.append(chunk_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_concat = pd.concat(content_en_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Mr. Speaker, I rise on a point of order. I believe you would find unanimous consent following consultation among all parties to further defer the vote just deferred until Monday to next Tuesday at the end of government orders.</p>'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting the english statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_column (row):\n",
    "    soup = BeautifulSoup(row)\n",
    "    text = soup.text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df.tokenized_content[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(word_tokens):\n",
    "    words = [w for w in word_tokens if not w in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remove_stopwords(word_tokens=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [ \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[text for text in doc.split()] for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texts is a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = df_concat[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_statements = statements.apply(tokenize_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 547 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary = corpora.Dictionary(tokenized_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gensim_env\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gensim_env\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;31m# update Dictionary with the document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         logger.info(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gensim_env\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \"\"\"\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mydict = corpora.Dictionary(statements.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-111032a7bbaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmydict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstatements\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-64-111032a7bbaf>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmydict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstatements\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\gensim_env\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \"\"\"\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "corpus = [mydict.doc2bow(line) for line in statements.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(statements.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = statements.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['mr.', 'speaker', ',', 'i', 'rise', 'on', 'a', 'point', 'of', 'order', '.', 'i', 'believe', 'you', 'would', 'find', 'unanimous', 'consent', 'following', 'consultation', 'among', 'all', 'parties', 'to', 'further', 'defer', 'the', 'vote', 'just', 'deferred', 'until', 'monday', 'to', 'next', 'tuesday', 'at', 'the', 'end', 'of', 'government', 'orders', '.']\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " \"'mr\",\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'speaker\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'i',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'rise\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'on\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'a',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'point\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'of\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'order\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'i',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'believe\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'you\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'would\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'find\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'unanimous\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'consent\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'following\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'consultation\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'among\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'all\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'parties\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'to\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'further\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'defer\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'the\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'vote\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'just\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'deferred\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'until\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'monday\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'to\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'next\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'tuesday\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'at\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'the\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'end\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'of\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'government\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'orders\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '.',\n",
       " \"'\",\n",
       " ']']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_column(df_concat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
